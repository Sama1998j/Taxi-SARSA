{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Before running the code, make sure you have the Gym library installed. You can install it using:\n",
        "\n",
        "```python\n",
        "!pip install gym\n"
      ],
      "metadata": {
        "id": "QoDns2QR8gVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "pSyHSeVE77i7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initialize_q_values function creates and returns a Q-table initialized with zeros, where each row corresponds to a state, and each column represents a possible action in a reinforcement learning environment. The dimensions of the Q-table are determined by the num_states (number of states) and num_actions (number of possible actions)."
      ],
      "metadata": {
        "id": "00cq92vg8hgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def initialize_q_values(num_states, num_actions):\n",
        "    return np.zeros([num_states, num_actions])"
      ],
      "metadata": {
        "id": "LghQKR_Y77tZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The epsilon_greedy_action function selects a random action with probability epsilon or chooses the action with the highest Q-value for the given state using the Q-table (q_values) and the action space (action_space)."
      ],
      "metadata": {
        "id": "ONj0Fbbp8rCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_action(q_values, state, epsilon, action_space):\n",
        "    return action_space.sample() if random.uniform(0, 1) < epsilon else np.argmax(q_values[state])"
      ],
      "metadata": {
        "id": "wUSPxnvb77wo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The update_q_values function implements the Q-value update rule for a state-action pair in a Q-table, incorporating the observed reward, the learning rate (learning_rate), and the discount factor (discount_factor). This update is based on the temporal difference error, adjusting the Q-value towards the estimated future rewards."
      ],
      "metadata": {
        "id": "HnKZkGyy8rfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_q_values(q_values, state, action, reward, next_state, next_action, learning_rate, discount_factor):\n",
        "    q_values[state, action] = q_values[state, action] + learning_rate * (reward + discount_factor * q_values[next_state, next_action] - q_values[state, action])"
      ],
      "metadata": {
        "id": "JpOwdOin77zP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_sarsa_agent function trains a SARSA (State-Action-Reward-State-Action) agent by iteratively interacting with the environment over a specified number of episodes (num_episodes). It utilizes an epsilon-greedy strategy to explore and update the Q-values based on observed rewards, learning rate (learning_rate), and discount factor (discount_factor). The final trained Q-values are returned for the agent to make informed decisions in the environment."
      ],
      "metadata": {
        "id": "-l_yb_JP_Dus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sarsa_agent(environment, num_episodes, learning_rate, discount_factor, exploration_rate):\n",
        "    state_space_size = environment.observation_space.n\n",
        "    action_space_size = environment.action_space.n\n",
        "    q_values = initialize_q_values(state_space_size, action_space_size)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        current_state = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        current_action = epsilon_greedy_action(q_values, current_state, exploration_rate, environment.action_space)\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done, _ = environment.step(current_action)\n",
        "\n",
        "            next_action = epsilon_greedy_action(q_values, next_state, exploration_rate, environment.action_space)\n",
        "\n",
        "            update_q_values(q_values, current_state, current_action, reward, next_state, next_action, learning_rate, discount_factor)\n",
        "\n",
        "            current_state = next_state\n",
        "            current_action = next_action\n",
        "\n",
        "    return q_values"
      ],
      "metadata": {
        "id": "RMWD_9Xb7714"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluate_agent function assesses the performance of a trained agent in an environment over multiple trials (num_trials). It calculates the average total reward obtained by executing actions based on the learned Q-values (q_values) and returns this average as a measure of the agent's effectiveness."
      ],
      "metadata": {
        "id": "4XYLXrfK_uDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(environment, q_values, num_trials=100):\n",
        "    total_rewards = 0\n",
        "    for _ in range(num_trials):\n",
        "        state = environment.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(q_values[state])\n",
        "            state, reward, done, _ = environment.step(action)\n",
        "            total_rewards += reward\n",
        "    return total_rewards / num_trials"
      ],
      "metadata": {
        "id": "h_Jj2saf774d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line creates a reinforcement learning environment using the Gym library, specifically the \"Taxi-v3\" environment. This environment models a simplified taxi navigation problem where an agent must pick up and drop off passengers."
      ],
      "metadata": {
        "id": "WtByp3IU_5sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Taxi-v3 environment\n",
        "taxi_environment = gym.make(\"Taxi-v3\")"
      ],
      "metadata": {
        "id": "3jS0N5av8O9v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameters define the learning characteristics of a SARSA (State-Action-Reward-State-Action) reinforcement learning agent. These parameters include the learning rate (`learning_rate`), discount factor (`discount_factor`), exploration rate (`exploration_rate`), and the total number of training episodes (`num_episodes`). Adjusting these values influences the agent's behavior and learning process during training."
      ],
      "metadata": {
        "id": "E4qf575y_7LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "exploration_rate = 0.1\n",
        "num_episodes = 1000"
      ],
      "metadata": {
        "id": "eI4yq2uP8RiK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line initiates the training of a SARSA (State-Action-Reward-State-Action) reinforcement learning agent on the \"Taxi-v3\" environment using specified hyperparameters. The resulting `trained_q_values` represent the learned state-action values, enabling the agent to make informed decisions in the environment."
      ],
      "metadata": {
        "id": "ZS3w76LQAReu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the SARSA agent\n",
        "trained_q_values = train_sarsa_agent(taxi_environment, num_episodes, learning_rate, discount_factor, exploration_rate)"
      ],
      "metadata": {
        "id": "FYH7C7928T_p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment evaluates the performance of a trained SARSA agent in the \"Taxi-v3\" environment over 100 trials. The average reward obtained during these trials is calculated and printed as a measure of the agent's effectiveness in navigating and completing tasks within the environment."
      ],
      "metadata": {
        "id": "E-2NraScAi1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained agent\n",
        "average_reward = evaluate_agent(taxi_environment, trained_q_values)\n",
        "print(f\"The trained agent achieves an average reward of {average_reward} over 100 trials.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhQvlEIA8V84",
        "outputId": "eead488d-28d7-4bce-f97b-0108f79b6017"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The trained agent achieves an average reward of -160.1 over 100 trials.\n"
          ]
        }
      ]
    }
  ]
}